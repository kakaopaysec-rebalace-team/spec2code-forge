import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Union
import logging
import asyncio
import json
from datetime import datetime, timedelta
import requests
import os
from dotenv import load_dotenv
import PyPDF2
import io
import arxiv
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
import sqlite3
from pathlib import Path
import xml.etree.ElementTree as ET
import re

# For AI model integration
try:
    import anthropic
    from anthropic import Anthropic
except ImportError:
    anthropic = None
    Anthropic = None

# For Ollama integration
try:
    import ollama
except ImportError:
    ollama = None

load_dotenv()
logger = logging.getLogger(__name__)

class AIModelTrainer:
    """
    AI ëª¨ë¸ í•™ìŠµ ë° ì „ëµ ìƒì„± ëª¨ë“ˆ
    Claude ëª¨ë¸ì„ í™œìš©í•œ ë¦¬ë°¸ëŸ°ì‹± ì „ëµ ìƒì„±
    ë‹¤ì–‘í•œ í•™ìŠµ ì†ŒìŠ¤ë¥¼ í†µí•œ AI ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •
    """
    
    def __init__(self):
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.google_search_api_key = os.getenv("GOOGLE_SEARCH_API_KEY")
        self.google_search_engine_id = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
        
        # Ollama settings
        self.ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
        self.ollama_model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")  # Default free model
        
        if self.anthropic_api_key and Anthropic:
            self.client = Anthropic(api_key=self.anthropic_api_key)
        else:
            self.client = None
            logger.warning("Anthropic API key not found or anthropic package not installed")
            
        # Check Ollama availability
        self.ollama_available = self._check_ollama_availability()
        
        # Initialize knowledge base
        self.knowledge_base = {
            "web_content": [],
            "ebooks": [],
            "papers": [],
            "expert_strategies": [],
            "user_data": [],
            "simulation_results": []
        }
        
        # Initialize local database for expert strategies
        self.expert_db_path = "expert_strategies.db"
        self._init_expert_database()

    async def initialize(self):
        """ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            # í•„ìš”í•œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‘ì—… ìˆ˜í–‰
            logger.info("AI Model Trainer ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            self._init_expert_database()
            
            # Database AI Engine ì´ˆê¸°í™” (Claude API ëŒ€ì‹  ìš°ì„  ì‚¬ìš©)
            try:
                from database_ai_engine import get_database_ai_engine
                await get_database_ai_engine()
                logger.info("âœ… Database AI Engine ì´ˆê¸°í™” ì™„ë£Œ - API í‚¤ ë¶ˆí•„ìš”")
            except Exception as e:
                logger.warning(f"Database AI Engine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            
            # Claude APIëŠ” ë³´ì¡°ì ìœ¼ë¡œë§Œ ì‚¬ìš© (ì˜¤ë¥˜ ë¡œê·¸ ìµœì†Œí™”)
            if self.client:
                logger.info("Claude API ì‚¬ìš© ê°€ëŠ¥ (ë³´ì¡° ë¶„ì„ìš©)")
            else:
                logger.info("Database AI Engine ë‹¨ë… ëª¨ë“œ - ì™„ì „ ìë¦½í˜• ì‹œìŠ¤í…œ")
            
            logger.info("AI Model Trainer ë¹„ë™ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"AI Model Trainer ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰

    def _init_expert_database(self):
        """ì „ë¬¸ê°€ ì „ëµ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.expert_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS expert_strategies (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    expert_name TEXT NOT NULL,
                    strategy_name TEXT NOT NULL,
                    investment_style TEXT NOT NULL,
                    allocation_json TEXT NOT NULL,
                    rationale TEXT,
                    performance_metrics TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS simulation_feedback (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    strategy_id INTEGER,
                    returns REAL,
                    volatility REAL,
                    sharpe_ratio REAL,
                    max_drawdown REAL,
                    feedback_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (strategy_id) REFERENCES expert_strategies (id)
                )
            ''')
            
            conn.commit()
            conn.close()
            
            # Add some default expert strategies
            self._add_default_expert_strategies()
            
        except Exception as e:
            logger.error(f"Error initializing expert database: {str(e)}")

    def _add_default_expert_strategies(self):
        """ê¸°ë³¸ ì „ë¬¸ê°€ ì „ëµ ì¶”ê°€"""
        default_strategies = [
            {
                "expert_name": "ì›ŒëŸ° ë²„í•",
                "strategy_name": "ê°€ì¹˜ íˆ¬ì ì „ëµ",
                "investment_style": "conservative",
                "allocation": {
                    "ì‚¼ì„±ì „ì": 0.25,
                    "Apple": 0.20,
                    "Berkshire Hathaway": 0.15,
                    "Johnson & Johnson": 0.15,
                    "Coca-Cola": 0.10,
                    "Cash": 0.15
                },
                "rationale": "ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ ë‚´ì¬ ê°€ì¹˜ê°€ ë†’ì€ ê¸°ì—…ì— íˆ¬ìí•˜ì—¬ ì•ˆì •ì ì¸ ìˆ˜ìµ ì¶”êµ¬",
                "performance_metrics": {"expected_return": 0.12, "volatility": 0.15, "sharpe_ratio": 0.8}
            },
            {
                "expert_name": "í”¼í„° ë¦°ì¹˜",
                "strategy_name": "ì„±ì¥ì£¼ íˆ¬ì ì „ëµ",
                "investment_style": "aggressive",
                "allocation": {
                    "NVIDIA": 0.20,
                    "Tesla": 0.15,
                    "Amazon": 0.15,
                    "Microsoft": 0.15,
                    "ì‚¼ì„±ì „ì": 0.15,
                    "NAVER": 0.10,
                    "ê¸°íƒ€ ì„±ì¥ì£¼": 0.10
                },
                "rationale": "ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” ê¸°ì—…ì— ì§‘ì¤‘ íˆ¬ìí•˜ì—¬ ë†’ì€ ìˆ˜ìµë¥  ì¶”êµ¬",
                "performance_metrics": {"expected_return": 0.18, "volatility": 0.22, "sharpe_ratio": 0.82}
            },
            {
                "expert_name": "ë ˆì´ ë‹¬ë¦¬ì˜¤",
                "strategy_name": "ì˜¬ì›¨ë” í¬íŠ¸í´ë¦¬ì˜¤",
                "investment_style": "moderate",
                "allocation": {
                    "ì£¼ì‹": 0.30,
                    "ì¤‘ê¸°ì±„ê¶Œ": 0.15,
                    "ì¥ê¸°ì±„ê¶Œ": 0.40,
                    "ì›ìì¬": 0.075,
                    "REITs": 0.075
                },
                "rationale": "ëª¨ë“  ê²½ì œ í™˜ê²½ì—ì„œ ì•ˆì •ì ì¸ ìˆ˜ìµì„ ì¶”êµ¬í•˜ëŠ” ë¶„ì‚° íˆ¬ì ì „ëµ",
                "performance_metrics": {"expected_return": 0.10, "volatility": 0.12, "sharpe_ratio": 0.83}
            }
        ]
        
        try:
            conn = sqlite3.connect(self.expert_db_path)
            cursor = conn.cursor()
            
            for strategy in default_strategies:
                cursor.execute('''
                    INSERT OR IGNORE INTO expert_strategies 
                    (expert_name, strategy_name, investment_style, allocation_json, rationale, performance_metrics)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    strategy["expert_name"],
                    strategy["strategy_name"],
                    strategy["investment_style"],
                    json.dumps(strategy["allocation"]),
                    strategy["rationale"],
                    json.dumps(strategy["performance_metrics"])
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"Error adding default expert strategies: {str(e)}")

    async def train_with_multiple_sources(
        self, 
        learning_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¥¼ í†µí•œ AI ëª¨ë¸ í•™ìŠµ
        
        Args:
            learning_config: í•™ìŠµ ì„¤ì •
            {
                "web_search_keywords": ["íˆ¬ì ì „ëµ", "í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”"],
                "ebook_urls": ["url1", "url2"],
                "research_keywords": ["quantitative finance", "portfolio optimization"],
                "expert_strategies": [strategy_data],
                "user_data": {text, files, urls},
                "enable_simulation_learning": True
            }
        """
        try:
            logger.info("Starting multi-source AI training")
            training_results = {
                "web_learning": {},
                "ebook_learning": {},
                "paper_learning": {},
                "expert_learning": {},
                "user_data_learning": {},
                "simulation_learning": {}
            }
            
            # 1. ì¸í„°ë„· ê²€ìƒ‰ì„ í†µí•œ í•™ìŠµ
            if learning_config.get("web_search_keywords"):
                training_results["web_learning"] = await self._learn_from_web_search(
                    learning_config["web_search_keywords"]
                )
            
            # 2. E-book ê²€ìƒ‰ ë° í•™ìŠµ
            if learning_config.get("ebook_urls"):
                training_results["ebook_learning"] = await self._learn_from_ebooks(
                    learning_config["ebook_urls"]
                )
            
            # 3. ë…¼ë¬¸ ê²€ìƒ‰ ë° í•™ìŠµ
            if learning_config.get("research_keywords"):
                training_results["paper_learning"] = await self._learn_from_papers(
                    learning_config["research_keywords"]
                )
            
            # 4. ì „ë¬¸ê°€ ì§ì ‘ ì…ë ¥ ì „ëµ í•™ìŠµ
            if learning_config.get("expert_strategies"):
                training_results["expert_learning"] = await self._learn_from_expert_strategies(
                    learning_config["expert_strategies"]
                )
            
            # 5. ì‚¬ìš©ì ì§ì ‘ ì…ë ¥ ë°ì´í„° í•™ìŠµ
            if learning_config.get("user_data"):
                training_results["user_data_learning"] = await self._learn_from_user_data(
                    learning_config["user_data"]
                )
            
            # 6. ëª¨ì˜ íˆ¬ì ë°ì´í„° ê¸°ë°˜ ê°•í™” í•™ìŠµ
            if learning_config.get("enable_simulation_learning"):
                training_results["simulation_learning"] = await self._learn_from_simulation_feedback()
            
            # í†µí•© í•™ìŠµ ê²°ê³¼ ìƒì„±
            integrated_knowledge = self._integrate_learning_results(training_results)
            
            logger.info("Multi-source AI training completed")
            return {
                "status": "success",
                "training_results": training_results,
                "integrated_knowledge": integrated_knowledge,
                "trained_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in multi-source training: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "trained_at": datetime.now().isoformat()
            }

    async def _learn_from_web_search(self, keywords: List[str]) -> Dict[str, Any]:
        """ì¸í„°ë„· ê²€ìƒ‰ì„ í†µí•œ í•™ìŠµ"""
        try:
            web_insights = []
            
            for keyword in keywords:
                # Google Search API ì‚¬ìš©
                if self.google_search_api_key:
                    search_results = await self._search_google_for_learning(keyword)
                    
                    for result in search_results:
                        # ì›¹ ì½˜í…ì¸  ì¶”ì¶œ ë° ë¶„ì„
                        content = await self._extract_web_content(result["url"])
                        if content and content.get("content"):
                            # Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ë¶„ì„
                            analysis = await self._analyze_content_with_claude(
                                content["content"], 
                                f"íˆ¬ì ì „ëµ ê´€ë ¨ ì½˜í…ì¸  ë¶„ì„: {keyword}"
                            )
                            
                            web_insights.append({
                                "keyword": keyword,
                                "url": result["url"],
                                "title": result["title"],
                                "analysis": analysis,
                                "extracted_at": datetime.now().isoformat()
                            })
                
                await asyncio.sleep(1)  # Rate limiting
            
            # ì›¹ í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["web_content"].extend(web_insights)
            
            return {
                "insights_count": len(web_insights),
                "keywords_processed": keywords,
                "key_learnings": self._extract_key_learnings(web_insights)
            }
            
        except Exception as e:
            logger.error(f"Error in web search learning: {str(e)}")
            return {"error": str(e)}

    async def _learn_from_ebooks(self, ebook_urls: List[str]) -> Dict[str, Any]:
        """E-book ê²€ìƒ‰ ë° í•™ìŠµ"""
        try:
            ebook_insights = []
            
            for url in ebook_urls:
                try:
                    # PDFë‚˜ í…ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
                    content = await self._download_and_extract_ebook(url)
                    
                    if content:
                        # ì±… ë‚´ìš©ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
                        chunks = self._split_content_into_chunks(content, max_chunk_size=8000)
                        
                        for i, chunk in enumerate(chunks[:5]):  # ì²˜ìŒ 5ê°œ ì²­í¬ë§Œ ì²˜ë¦¬
                            analysis = await self._analyze_content_with_claude(
                                chunk,
                                "íˆ¬ì ì „ë¬¸ì„œì  ë‚´ìš© ë¶„ì„ ë° í•µì‹¬ íˆ¬ì ì›ì¹™ ì¶”ì¶œ"
                            )
                            
                            ebook_insights.append({
                                "url": url,
                                "chunk_index": i,
                                "analysis": analysis,
                                "processed_at": datetime.now().isoformat()
                            })
                    
                    await asyncio.sleep(2)  # Rate limiting
                    
                except Exception as e:
                    logger.warning(f"Failed to process ebook {url}: {str(e)}")
                    continue
            
            # E-book í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["ebooks"].extend(ebook_insights)
            
            return {
                "insights_count": len(ebook_insights),
                "ebooks_processed": len(ebook_urls),
                "key_learnings": self._extract_key_learnings(ebook_insights)
            }
            
        except Exception as e:
            logger.error(f"Error in ebook learning: {str(e)}")
            return {"error": str(e)}

    async def _learn_from_papers(self, research_keywords: List[str]) -> Dict[str, Any]:
        """ë…¼ë¬¸ ê²€ìƒ‰ ë° í•™ìŠµ"""
        try:
            paper_insights = []
            
            for keyword in research_keywords:
                try:
                    # arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ê²€ìƒ‰
                    papers = await self._search_arxiv_papers(keyword)
                    
                    for paper in papers[:3]:  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 3í¸
                        # ë…¼ë¬¸ ìš”ì•½ ë¶„ì„
                        analysis = await self._analyze_content_with_claude(
                            paper["summary"],
                            f"ê¸ˆìœµ ì—°êµ¬ ë…¼ë¬¸ ë¶„ì„: {keyword} ê´€ë ¨ í•™ìˆ  ì—°êµ¬ ë‚´ìš© ìš”ì•½ ë° ì‹¤ìš©ì  íˆ¬ì ì „ëµ ë„ì¶œ"
                        )
                        
                        paper_insights.append({
                            "keyword": keyword,
                            "title": paper["title"],
                            "authors": paper["authors"],
                            "summary": paper["summary"],
                            "url": paper["url"],
                            "analysis": analysis,
                            "processed_at": datetime.now().isoformat()
                        })
                
                except Exception as e:
                    logger.warning(f"Failed to process paper for keyword {keyword}: {str(e)}")
                    continue
                
                await asyncio.sleep(1)  # Rate limiting
            
            # ë…¼ë¬¸ í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["papers"].extend(paper_insights)
            
            return {
                "insights_count": len(paper_insights),
                "keywords_processed": research_keywords,
                "key_learnings": self._extract_key_learnings(paper_insights)
            }
            
        except Exception as e:
            logger.error(f"Error in paper learning: {str(e)}")
            return {"error": str(e)}

    async def _learn_from_expert_strategies(self, expert_strategies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì „ë¬¸ê°€ ì§ì ‘ ì…ë ¥ ì „ëµ í•™ìŠµ"""
        try:
            processed_count = 0
            
            conn = sqlite3.connect(self.expert_db_path)
            cursor = conn.cursor()
            
            for strategy in expert_strategies:
                try:
                    cursor.execute('''
                        INSERT INTO expert_strategies 
                        (expert_name, strategy_name, investment_style, allocation_json, rationale, performance_metrics)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        strategy.get("expert_name", "Unknown"),
                        strategy.get("strategy_name", "Custom Strategy"),
                        strategy.get("investment_style", "moderate"),
                        json.dumps(strategy.get("allocation", {})),
                        strategy.get("rationale", ""),
                        json.dumps(strategy.get("performance_metrics", {}))
                    ))
                    processed_count += 1
                    
                except Exception as e:
                    logger.warning(f"Failed to process expert strategy: {str(e)}")
                    continue
            
            conn.commit()
            conn.close()
            
            # ì „ë¬¸ê°€ ì „ëµ í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["expert_strategies"].extend(expert_strategies)
            
            return {
                "strategies_processed": processed_count,
                "total_strategies": len(expert_strategies),
                "success_rate": processed_count / len(expert_strategies) if expert_strategies else 0
            }
            
        except Exception as e:
            logger.error(f"Error in expert strategy learning: {str(e)}")
            return {"error": str(e)}

    async def _learn_from_user_data(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ì ‘ ì…ë ¥ ë°ì´í„° í•™ìŠµ"""
        try:
            user_insights = []
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
            if user_data.get("text"):
                text_analysis = await self._analyze_content_with_claude(
                    user_data["text"],
                    "ì‚¬ìš©ì íˆ¬ì ì² í•™ ë° ì„ í˜¸ë„ ë¶„ì„, ê°œì¸í™”ëœ íˆ¬ì ì „ëµ ìš”ì†Œ ì¶”ì¶œ"
                )
                user_insights.append({
                    "type": "text",
                    "content": user_data["text"],
                    "analysis": text_analysis,
                    "processed_at": datetime.now().isoformat()
                })
            
            # PDF íŒŒì¼ ì²˜ë¦¬
            if user_data.get("pdf_files"):
                for pdf_content in user_data["pdf_files"]:
                    pdf_text = self._extract_text_from_pdf(pdf_content)
                    if pdf_text:
                        pdf_analysis = await self._analyze_content_with_claude(
                            pdf_text[:8000],  # ì²˜ìŒ 8000ìë§Œ ë¶„ì„
                            "ì‚¬ìš©ì ì œê³µ PDF ë¬¸ì„œ ë¶„ì„ ë° íˆ¬ì ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"
                        )
                        user_insights.append({
                            "type": "pdf",
                            "content": pdf_text[:1000],  # ìš”ì•½ìš©
                            "analysis": pdf_analysis,
                            "processed_at": datetime.now().isoformat()
                        })
            
            # URL ì½˜í…ì¸  ì²˜ë¦¬
            if user_data.get("urls"):
                for url in user_data["urls"]:
                    url_content = await self._extract_web_content(url)
                    if url_content and url_content.get("content"):
                        url_analysis = await self._analyze_content_with_claude(
                            url_content["content"][:8000],
                            "ì‚¬ìš©ì ì œê³µ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë¶„ì„ ë° íˆ¬ì ì „ëµ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ"
                        )
                        user_insights.append({
                            "type": "url",
                            "url": url,
                            "content": url_content["content"][:1000],
                            "analysis": url_analysis,
                            "processed_at": datetime.now().isoformat()
                        })
            
            # ì‚¬ìš©ì ë°ì´í„° í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["user_data"].extend(user_insights)
            
            return {
                "insights_generated": len(user_insights),
                "data_types_processed": list(set(insight["type"] for insight in user_insights)),
                "key_learnings": self._extract_key_learnings(user_insights)
            }
            
        except Exception as e:
            logger.error(f"Error in user data learning: {str(e)}")
            return {"error": str(e)}

    async def _learn_from_simulation_feedback(self) -> Dict[str, Any]:
        """ëª¨ì˜ íˆ¬ì ë°ì´í„° ê¸°ë°˜ ê°•í™” í•™ìŠµ"""
        try:
            conn = sqlite3.connect(self.expert_db_path)
            cursor = conn.cursor()
            
            # ì‹œë®¬ë ˆì´ì…˜ í”¼ë“œë°± ë°ì´í„° ì¡°íšŒ
            cursor.execute('''
                SELECT es.*, sf.returns, sf.volatility, sf.sharpe_ratio, sf.max_drawdown, sf.feedback_score
                FROM expert_strategies es
                JOIN simulation_feedback sf ON es.id = sf.strategy_id
                ORDER BY sf.feedback_score DESC
            ''')
            
            feedback_data = cursor.fetchall()
            conn.close()
            
            if not feedback_data:
                return {"message": "No simulation feedback data available"}
            
            # ì„±ê³¼ê°€ ì¢‹ì€ ì „ëµë“¤ ë¶„ì„
            top_strategies = feedback_data[:5]  # ìƒìœ„ 5ê°œ ì „ëµ
            
            performance_analysis = []
            for strategy in top_strategies:
                strategy_dict = {
                    "strategy_name": strategy[2],
                    "investment_style": strategy[3],
                    "allocation": json.loads(strategy[4]),
                    "returns": strategy[7],
                    "volatility": strategy[8],
                    "sharpe_ratio": strategy[9],
                    "max_drawdown": strategy[10],
                    "feedback_score": strategy[11]
                }
                performance_analysis.append(strategy_dict)
            
            # Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ê³µ íŒ¨í„´ ë¶„ì„
            patterns_analysis = await self._analyze_content_with_claude(
                json.dumps(performance_analysis, indent=2),
                "ì„±ê³µí•œ íˆ¬ì ì „ëµë“¤ì˜ ê³µí†µ íŒ¨í„´ ë¶„ì„ ë° í–¥í›„ ì „ëµ ê°œì„  ë°©í–¥ ì œì‹œ"
            )
            
            # ê°•í™” í•™ìŠµ ê²°ê³¼ ì €ì¥
            self.knowledge_base["simulation_results"].append({
                "top_strategies": performance_analysis,
                "patterns_analysis": patterns_analysis,
                "analyzed_at": datetime.now().isoformat()
            })
            
            return {
                "strategies_analyzed": len(feedback_data),
                "top_performers": len(top_strategies),
                "patterns_identified": patterns_analysis,
                "improvement_suggestions": "Based on historical performance data"
            }
            
        except Exception as e:
            logger.error(f"Error in simulation feedback learning: {str(e)}")
            return {"error": str(e)}

    async def generate_strategy(
        self, 
        processed_data: pd.DataFrame,
        user_profile: Dict[str, Any],
        user_uploaded_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ë¦¬ë°¸ëŸ°ì‹± ì „ëµ ìƒì„±
        
        Args:
            processed_data: data_processor.pyì—ì„œ ìƒì„±ëœ ë°ì´í„°í”„ë ˆì„
            user_profile: ì‚¬ìš©ì ì…ë ¥ ì •ë³´ (íˆ¬ì ì„±í–¥, ëª©í‘œ ìˆ˜ìµë¥ , íˆ¬ì ê¸°ê°„ ë“±)
            user_uploaded_data: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œ, URL ë“±ì˜ ë°ì´í„°
            
        Returns:
            rebalancing_strategy: ë¦¬ë°¸ëŸ°ì‹± ì „ëµ (JSON í˜•ì‹)
            portfolio_allocation: ëª©í‘œ í¬íŠ¸í´ë¦¬ì˜¤ ìì‚° ë°°ë¶„ ë¹„ì¤‘
            actions: ì¶”ì²œ ë§¤ìˆ˜/ë§¤ë„ ì¢…ëª© ë° ìˆ˜ëŸ‰
            rationale: AIê°€ í•´ë‹¹ ì „ëµì„ ì œì•ˆí•œ ì´ìœ ì— ëŒ€í•œ ì„¤ëª…
        """
        try:
            logger.info("Generating comprehensive AI rebalancing strategy")
            
            # 1. ì‚¬ìš©ì ì—…ë¡œë“œ ë°ì´í„° ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            user_insights = {}
            if user_uploaded_data:
                user_insights = await self._learn_from_user_data(user_uploaded_data)
            
            # 2. ì¢…í•© ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            comprehensive_context = await self._prepare_comprehensive_context(
                processed_data, user_profile, user_insights
            )
            
            # 3. AI ì „ëµ ìƒì„± (Database AI ìš°ì„ )
            try:
                from database_ai_engine import get_database_ai_engine
                db_ai = await get_database_ai_engine()
                
                # í˜„ì¬ ë³´ìœ ì¢…ëª© ë³€í™˜
                current_holdings = []
                if not processed_data.empty:
                    for _, row in processed_data.iterrows():
                        current_holdings.append({
                            'symbol': row.get('Symbol', ''),
                            'name': row.get('Name', ''),
                            'weight': row.get('Weight', 0.0) / 100.0 if 'Weight' in row else 0.0
                        })
                
                # Database AIë¡œ ì „ëµ ìƒì„±
                strategy = await db_ai.generate_intelligent_strategy(
                    user_profile, 
                    current_holdings,
                    {'market_data': processed_data.to_dict('records') if not processed_data.empty else []}
                )
                
                logger.info("âœ… Database AI ì „ëµ ìƒì„± ì„±ê³µ")
                
            except Exception as db_ai_error:
                logger.warning(f"Database AI ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {db_ai_error}")
                
                # ê¸°ì¡´ ë°©ì‹ í´ë°± (ë¬´ë£Œ LLM ìš°ì„ , Claude ë³´ì¡°)
                try:
                    if self.ollama_available:
                        logger.info("ğŸ¤– Ollama ë¬´ë£Œ LLMìœ¼ë¡œ ì „ëµ ìƒì„± ì¤‘...")
                        strategy = await self._generate_strategy_with_ollama(comprehensive_context)
                    elif self.client:
                        logger.info("ğŸ§  Claude APIë¡œ ì „ëµ ìƒì„± ì¤‘...")
                        strategy = await self._generate_advanced_strategy_with_claude(comprehensive_context)
                    else:
                        logger.info("ğŸ“Š ê·œì¹™ ê¸°ë°˜ ì „ëµ ìƒì„± ì¤‘...")
                        strategy = await self._generate_enhanced_rule_based_strategy(
                            processed_data, user_profile, user_insights
                        )
                except Exception as ai_error:
                    logger.warning(f"AI ì „ëµ ìƒì„± ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±: {ai_error}")
                    strategy = await self._generate_enhanced_rule_based_strategy(
                        processed_data, user_profile, user_insights
                    )
            
            # 4. ì „ëµ ê²€ì¦ ë° ìµœì í™”
            validated_strategy = await self._validate_and_optimize_strategy(strategy, processed_data)
            
            # 5. ë¦¬ìŠ¤í¬ ë¶„ì„ ì¶”ê°€
            risk_analysis = await self._perform_risk_analysis(validated_strategy, processed_data)
            validated_strategy["risk_analysis"] = risk_analysis
            
            logger.info("Advanced AI strategy generation completed")
            return validated_strategy
            
        except Exception as e:
            logger.error(f"Error generating advanced strategy: {str(e)}")
            return self._generate_fallback_strategy(user_profile)

    async def _prepare_comprehensive_context(
        self, 
        processed_data: pd.DataFrame,
        user_profile: Dict[str, Any],
        user_insights: Dict[str, Any]
    ) -> str:
        """ì¢…í•©ì ì¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        try:
            context_parts = []
            
            # 1. ì‚¬ìš©ì í”„ë¡œí•„
            context_parts.append("=== íˆ¬ìì í”„ë¡œí•„ ===")
            context_parts.append(f"íˆ¬ì ì„±í–¥: {user_profile.get('investment_style', 'moderate')}")
            context_parts.append(f"ëª©í‘œ ìˆ˜ìµë¥ : {user_profile.get('target_return', '10-15%')}")
            context_parts.append(f"íˆ¬ì ê¸°ê°„: {user_profile.get('investment_period', 'medium')}")
            context_parts.append(f"ìœ„í—˜ í—ˆìš©ë„: {user_profile.get('risk_tolerance', 'medium')}")
            context_parts.append(f"íˆ¬ì ëª©ì : {user_profile.get('investment_goal', 'wealth_building')}")
            
            # 2. ì‹œì¥ ë°ì´í„° ë¶„ì„
            if not processed_data.empty:
                context_parts.append("\n=== ì‹œì¥ ë°ì´í„° ë¶„ì„ ===")
                
                # ìµœì‹  ì‹œì¥ ìƒí™©
                latest_data = processed_data.groupby('Symbol').tail(1)
                for _, row in latest_data.head(10).iterrows():  # ìƒìœ„ 10ê°œ ì¢…ëª©
                    symbol = row.get('Symbol', 'Unknown')
                    price = row.get('Close', 0)
                    daily_return = row.get('Daily_Return', 0) * 100
                    volatility = row.get('Volatility_30D', 0) * 100
                    rsi = row.get('RSI', 50)
                    
                    context_parts.append(
                        f"{symbol}: í˜„ì¬ê°€ {price:.2f}, ì¼ì¼ìˆ˜ìµë¥  {daily_return:.2f}%, "
                        f"ë³€ë™ì„± {volatility:.2f}%, RSI {rsi:.1f}"
                    )
                
                # ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„
                context_parts.append("\nì‹œì¥ íŠ¸ë Œë“œ:")
                market_trend = self._analyze_market_trend(processed_data)
                context_parts.append(market_trend)
            
            # 3. ì§€ì‹ ë² ì´ìŠ¤ í™œìš©
            context_parts.append("\n=== í•™ìŠµëœ íˆ¬ì ì§€ì‹ ===")
            
            # ì›¹ í•™ìŠµ ë‚´ìš©
            if self.knowledge_base["web_content"]:
                recent_web_insights = self.knowledge_base["web_content"][-3:]  # ìµœê·¼ 3ê°œ
                context_parts.append("ìµœì‹  ì›¹ ë¶„ì„ ê²°ê³¼:")
                for insight in recent_web_insights:
                    if insight.get("analysis"):
                        context_parts.append(f"- {insight['analysis'][:200]}...")
            
            # ì „ë¬¸ê°€ ì „ëµ
            expert_strategies = self._get_relevant_expert_strategies(user_profile.get('investment_style', 'moderate'))
            if expert_strategies:
                context_parts.append("ê´€ë ¨ ì „ë¬¸ê°€ ì „ëµ:")
                for strategy in expert_strategies[:2]:  # ìƒìœ„ 2ê°œ
                    context_parts.append(f"- {strategy['expert_name']}: {strategy['rationale'][:150]}...")
            
            # 4. ì‚¬ìš©ì ê°œì¸í™” ë°ì´í„°
            if user_insights and user_insights.get("key_learnings"):
                context_parts.append("\n=== ì‚¬ìš©ì ë§ì¶¤ ë¶„ì„ ===")
                for learning in user_insights["key_learnings"][:3]:
                    context_parts.append(f"- {learning}")
            
            # 5. ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ê²°ê³¼
            if self.knowledge_base["simulation_results"]:
                latest_simulation = self.knowledge_base["simulation_results"][-1]
                context_parts.append("\n=== ì„±ê³¼ ë¶„ì„ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ===")
                context_parts.append(latest_simulation.get("patterns_identified", "")[:300])
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.error(f"Error preparing comprehensive context: {str(e)}")
            return "ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."

    async def _generate_advanced_strategy_with_claude(self, context: str) -> Dict[str, Any]:
        """Claudeë¥¼ ì‚¬ìš©í•œ ê³ ë„í™”ëœ ì „ëµ ìƒì„±"""
        try:
            prompt = f"""
ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¢…í•© ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

1. **ì¶”ì²œ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘** (JSON í˜•ì‹ìœ¼ë¡œ ì •í™•í•œ ì¢…ëª©ëª…ê³¼ ë¹„ì¤‘):
{{
    "ì‚¼ì„±ì „ì": 0.25,
    "Apple": 0.20,
    "NVIDIA": 0.15,
    ...
}}

2. **ë§¤ìˆ˜/ë§¤ë„ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸**:
- ë§¤ìˆ˜: [ì¢…ëª©ëª…] - [ì´ìœ ] - [ëª©í‘œë¹„ì¤‘]
- ë§¤ë„: [ì¢…ëª©ëª…] - [ì´ìœ ] - [í˜„ì¬ë¹„ì¤‘ â†’ ëª©í‘œë¹„ì¤‘]

3. **ì „ëµ í•µì‹¬ ê·¼ê±°**:
- ì‹œì¥ ìƒí™© ë¶„ì„
- ì‚¬ìš©ì í”„ë¡œí•„ ë¶€í•©ì„±
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì•ˆ

4. **ì„±ê³¼ ì˜ˆì¸¡**:
- ì˜ˆìƒ ì—°ìˆ˜ìµë¥ : [ë²”ìœ„]
- ì˜ˆìƒ ë³€ë™ì„±: [ìˆ˜ì¹˜]
- ìµœëŒ€ ì†ì‹¤ ê°€ëŠ¥ì„±: [MDD]

5. **êµ¬ì²´ì  ì‹¤í–‰ ê³„íš**:
- ë‹¨ê³„ë³„ ë¦¬ë°¸ëŸ°ì‹± ë°©ë²•
- ëª¨ë‹ˆí„°ë§ ì§€í‘œ
- ì¬ì¡°ì • ì‹œì 

ì‘ë‹µì€ í•œêµ­ì–´ë¡œ, ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
            
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            ai_response = response.content[0].text
            strategy = self._parse_advanced_ai_response(ai_response)
            
            logger.info("Successfully generated advanced strategy with Claude")
            return strategy
            
        except Exception as e:
            logger.error(f"Error with advanced Claude API: {str(e)}")
            raise
            
    def _check_ollama_availability(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ê°€ìš©ì„± ì²´í¬"""
        try:
            if not ollama:
                logger.info("Ollama package not installed")
                return False
                
            # Simple health check
            import requests
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("Ollama service is available")
                return True
            else:
                logger.warning(f"Ollama service returned status {response.status_code}")
                return False
        except Exception as e:
            logger.info(f"Ollama not available: {str(e)}")
            return False
    
    async def _generate_strategy_with_ollama(self, context: str) -> Dict[str, Any]:
        """ë¬´ë£Œ Ollama LLMì„ ì‚¬ìš©í•œ ì „ëµ ìƒì„±"""
        try:
            if not self.ollama_available:
                raise Exception("Ollama is not available")
                
            prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

**ì¶”ì²œ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘**
- ì‚¼ì„±ì „ì: 25%
- Apple: 20%  
- NVIDIA: 15%
- ê¸°íƒ€...

**ì£¼ìš” ì•¡ì…˜**
- ë§¤ìˆ˜: [ì¢…ëª©ëª…] - [ì´ìœ ]
- ë§¤ë„: [ì¢…ëª©ëª…] - [ì´ìœ ]

**ì „ëµ ê·¼ê±°**
- í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„
- ì‚¬ìš©ì í”„ë¡œí•„ì— ë§ëŠ” ì´ìœ 
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì•ˆ

**ì„±ê³¼ ì˜ˆì¸¡**
- ì˜ˆìƒ ì—°ìˆ˜ìµë¥ : 10-15%
- ì˜ˆìƒ ë³€ë™ì„±: 15-20%
- ìµœëŒ€ ì†ì‹¤: 10-15%

í•œêµ­ì–´ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."""

            # Call Ollama API
            import requests
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "max_tokens": 2000
                    }
                },
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.status_code}")
                
            result = response.json()
            ai_response = result.get("response", "")
            
            if not ai_response:
                raise Exception("Empty response from Ollama")
                
            # Parse the response
            strategy = self._parse_ollama_response(ai_response)
            
            logger.info("Successfully generated strategy with Ollama")
            return strategy
            
        except Exception as e:
            logger.error(f"Error with Ollama API: {str(e)}")
            # Fallback to rule-based strategy
            raise
    
    def _parse_ollama_response(self, ai_response: str) -> Dict[str, Any]:
        """Ollama ì‘ë‹µ íŒŒì‹±"""
        try:
            # Extract portfolio allocation
            portfolio_allocation = {}
            lines = ai_response.split('\n')
            
            in_portfolio_section = False
            for line in lines:
                line = line.strip()
                
                if "ì¶”ì²œ í¬íŠ¸í´ë¦¬ì˜¤" in line or "í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘" in line:
                    in_portfolio_section = True
                    continue
                elif "ì£¼ìš” ì•¡ì…˜" in line or "ì „ëµ ê·¼ê±°" in line:
                    in_portfolio_section = False
                    continue
                    
                if in_portfolio_section and line and ":" in line:
                    parts = line.replace("-", "").strip().split(":")
                    if len(parts) == 2:
                        stock = parts[0].strip()
                        weight_str = parts[1].strip().replace("%", "").replace(" ", "")
                        try:
                            weight = float(weight_str) / 100.0
                            if 0 <= weight <= 1:
                                portfolio_allocation[stock] = weight
                        except ValueError:
                            continue
            
            # Extract actions
            actions = self._extract_actions_from_response(ai_response)
            
            # Extract rationale
            rationale = self._extract_rationale(ai_response)
            
            # Extract performance predictions
            performance = self._extract_performance_predictions(ai_response)
            
            return {
                "portfolio_allocation": portfolio_allocation,
                "actions": actions,
                "rationale": rationale,
                "expected_return": performance.get("return", "10-15%"),
                "expected_volatility": performance.get("volatility", "15-20%"),
                "max_drawdown": performance.get("mdd", "10-15%"),
                "risk_level": self._determine_risk_level(portfolio_allocation),
                "generated_at": datetime.now().isoformat(),
                "strategy_type": "ollama_free"
            }
            
        except Exception as e:
            logger.error(f"Error parsing Ollama response: {str(e)}")
            return self._create_fallback_parsed_response(ai_response)

    def _parse_advanced_ai_response(self, ai_response: str) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ AI ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON íŒ¨í„´ ì¶”ì¶œ
            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            json_matches = re.findall(json_pattern, ai_response)
            
            portfolio_allocation = {}
            if json_matches:
                try:
                    portfolio_allocation = json.loads(json_matches[0])
                except json.JSONDecodeError:
                    # ìˆ˜ë™ íŒŒì‹± ì‹œë„
                    portfolio_allocation = self._manual_parse_allocation(json_matches[0])
            
            # ì•¡ì…˜ ì¶”ì¶œ
            actions = self._extract_actions_from_response(ai_response)
            
            # ì„±ê³¼ ì˜ˆì¸¡ ì¶”ì¶œ
            performance = self._extract_performance_predictions(ai_response)
            
            # ì „ëµ ê·¼ê±° ì¶”ì¶œ
            rationale = self._extract_rationale(ai_response)
            
            return {
                "portfolio_allocation": portfolio_allocation,
                "actions": actions,
                "rationale": rationale,
                "expected_return": performance.get("return", "10-15%"),
                "expected_volatility": performance.get("volatility", "15-20%"),
                "max_drawdown": performance.get("mdd", "10-15%"),
                "risk_level": self._determine_risk_level(portfolio_allocation),
                "implementation_plan": self._extract_implementation_plan(ai_response),
                "monitoring_indicators": self._extract_monitoring_indicators(ai_response),
                "generated_at": datetime.now().isoformat(),
                "strategy_type": "ai_advanced"
            }
            
        except Exception as e:
            logger.error(f"Error parsing advanced AI response: {str(e)}")
            return self._create_fallback_parsed_response(ai_response)

    def _manual_parse_allocation(self, json_text: str) -> Dict[str, float]:
        """ìˆ˜ë™ìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ë°°ë¶„ íŒŒì‹±"""
        allocation = {}
        lines = json_text.split('\n')
        
        for line in lines:
            if ':' in line and any(char.isdigit() for char in line):
                parts = line.split(':')
                if len(parts) == 2:
                    stock = parts[0].strip().replace('"', '').replace('{', '').replace(',', '')
                    weight_str = parts[1].strip().replace('"', '').replace('}', '').replace(',', '')
                    
                    try:
                        weight = float(weight_str)
                        if weight > 1:  # ë°±ë¶„ìœ¨ë¡œ í‘œì‹œëœ ê²½ìš°
                            weight = weight / 100
                        allocation[stock] = weight
                    except ValueError:
                        continue
        
        return allocation

    def _extract_actions_from_response(self, response: str) -> List[Dict[str, Any]]:
        """ì‘ë‹µì—ì„œ ì•¡ì…˜ ì¶”ì¶œ"""
        actions = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith(('ë§¤ìˆ˜:', '- ë§¤ìˆ˜:', 'ë§¤ë„:', '- ë§¤ë„:')):
                action_type = "ë§¤ìˆ˜" if "ë§¤ìˆ˜" in line else "ë§¤ë„"
                
                # ì¢…ëª©ëª…ê³¼ ì´ìœ  ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
                parts = line.replace('ë§¤ìˆ˜:', '').replace('ë§¤ë„:', '').replace('- ', '').split(' - ')
                
                if parts:
                    stock = parts[0].strip()
                    reason = parts[1] if len(parts) > 1 else "AI ì¶”ì²œ"
                    target_weight = parts[2] if len(parts) > 2 else "ì ì • ë¹„ì¤‘"
                    
                    actions.append({
                        "action": action_type,
                        "stock": stock,
                        "reason": reason,
                        "target_weight": target_weight
                    })
        
        return actions

    def _extract_performance_predictions(self, response: str) -> Dict[str, str]:
        """ì„±ê³¼ ì˜ˆì¸¡ ì¶”ì¶œ"""
        performance = {}
        lines = response.split('\n')
        
        for line in lines:
            line = line.lower()
            if 'ìˆ˜ìµë¥ ' in line and '%' in line:
                performance["return"] = self._extract_percentage_from_line(line)
            elif 'ë³€ë™ì„±' in line and '%' in line:
                performance["volatility"] = self._extract_percentage_from_line(line)
            elif 'mdd' in line or 'ìµœëŒ€' in line and 'ì†ì‹¤' in line:
                performance["mdd"] = self._extract_percentage_from_line(line)
        
        return performance

    def _extract_percentage_from_line(self, line: str) -> str:
        """ë¼ì¸ì—ì„œ í¼ì„¼íŠ¸ ì¶”ì¶œ"""
        import re
        percentages = re.findall(r'\d+(?:\.\d+)?%', line)
        if percentages:
            return percentages[0]
        
        # ë²”ìœ„ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: 10-15%)
        range_pattern = re.findall(r'\d+(?:\.\d+)?-\d+(?:\.\d+)?%', line)
        if range_pattern:
            return range_pattern[0]
        
        return "ì •ë³´ ì—†ìŒ"

    def _extract_rationale(self, response: str) -> str:
        """ì „ëµ ê·¼ê±° ì¶”ì¶œ"""
        lines = response.split('\n')
        rationale_section = False
        rationale_lines = []
        
        for line in lines:
            if 'ê·¼ê±°' in line or 'ì´ìœ ' in line or 'ì „ëµ' in line:
                rationale_section = True
                continue
            elif rationale_section and line.strip():
                if line.startswith(('4.', '5.', '#')):  # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘
                    break
                rationale_lines.append(line.strip())
        
        return ' '.join(rationale_lines) if rationale_lines else response[:500]

    def _extract_implementation_plan(self, response: str) -> List[str]:
        """ì‹¤í–‰ ê³„íš ì¶”ì¶œ"""
        lines = response.split('\n')
        plan_section = False
        plan_items = []
        
        for line in lines:
            if 'ì‹¤í–‰' in line or 'êµ¬ì²´ì ' in line:
                plan_section = True
                continue
            elif plan_section and line.strip():
                if line.startswith('-') or line.strip().startswith('â€¢'):
                    plan_items.append(line.strip())
                elif not line[0].isdigit() and not line.startswith('-'):
                    break
        
        return plan_items

    def _extract_monitoring_indicators(self, response: str) -> List[str]:
        """ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì¶”ì¶œ"""
        indicators = []
        if 'ëª¨ë‹ˆí„°ë§' in response:
            lines = response.split('\n')
            monitoring_section = False
            
            for line in lines:
                if 'ëª¨ë‹ˆí„°ë§' in line:
                    monitoring_section = True
                    continue
                elif monitoring_section and line.strip():
                    if line.startswith('-') or 'ì§€í‘œ' in line:
                        indicators.append(line.strip())
        
        # ê¸°ë³¸ ì§€í‘œë“¤
        if not indicators:
            indicators = [
                "ì›”ë³„ ì„±ê³¼ ê²€í† ",
                "í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± (ë¶„ê¸°ë³„)",
                "ë¦¬ìŠ¤í¬ ì§€í‘œ ëª¨ë‹ˆí„°ë§",
                "ì‹œì¥ ìƒí™© ë³€í™” ì¶”ì "
            ]
        
        return indicators

    def _determine_risk_level(self, portfolio_allocation: Dict[str, float]) -> str:
        """í¬íŠ¸í´ë¦¬ì˜¤ ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •"""
        if not portfolio_allocation:
            return "ì¤‘ê°„"
        
        # ê³ ìœ„í—˜ ìì‚° ë¹„ì¤‘ ê³„ì‚°
        high_risk_assets = ["Tesla", "NVIDIA", "ë¹„íŠ¸ì½”ì¸", "ì„±ì¥ì£¼", "ì‹ í¥ì‹œì¥"]
        high_risk_weight = sum(
            weight for stock, weight in portfolio_allocation.items()
            if any(risk_asset in stock for risk_asset in high_risk_assets)
        )
        
        if high_risk_weight > 0.4:
            return "ë†’ìŒ"
        elif high_risk_weight > 0.2:
            return "ì¤‘ê°„"
        else:
            return "ë‚®ìŒ"

    async def _generate_enhanced_rule_based_strategy(
        self, 
        processed_data: pd.DataFrame,
        user_profile: Dict[str, Any],
        user_insights: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê°•í™”ëœ ê·œì¹™ ê¸°ë°˜ ì „ëµ (AI ë°±ì—…ìš©)"""
        try:
            investment_style = user_profile.get('investment_style', 'moderate')
            target_return = user_profile.get('target_return', 'moderate')
            
            # ê¸°ë³¸ ìì‚° ë°°ë¶„
            base_allocations = {
                'conservative': {
                    "ì‚¼ì„±ì „ì": 0.20, "Apple": 0.15, "Microsoft": 0.10,
                    "Johnson & Johnson": 0.10, "Coca-Cola": 0.10,
                    "ì±„ê¶ŒETF": 0.25, "í˜„ê¸ˆ": 0.10
                },
                'moderate': {
                    "ì‚¼ì„±ì „ì": 0.18, "Apple": 0.15, "Microsoft": 0.12,
                    "NVIDIA": 0.12, "Amazon": 0.08, "Google": 0.08,
                    "NAVER": 0.08, "SKí•˜ì´ë‹‰ìŠ¤": 0.10, "ì±„ê¶ŒETF": 0.09
                },
                'aggressive': {
                    "NVIDIA": 0.20, "Tesla": 0.15, "Apple": 0.12,
                    "Amazon": 0.10, "ì‚¼ì„±ì „ì": 0.13, "TSMC": 0.10,
                    "NAVER": 0.08, "ì„±ì¥ì£¼ETF": 0.12
                }
            }
            
            # ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ì¡°ì •
            if not processed_data.empty:
                market_adjustment = self._calculate_market_adjustment(processed_data)
                allocation = self._adjust_allocation_for_market(
                    base_allocations[investment_style], 
                    market_adjustment
                )
            else:
                allocation = base_allocations[investment_style]
            
            # ì‚¬ìš©ì ì¸ì‚¬ì´íŠ¸ ë°˜ì˜
            if user_insights and user_insights.get("key_learnings"):
                allocation = self._adjust_for_user_insights(allocation, user_insights)
            
            # ì•¡ì…˜ ìƒì„±
            actions = self._generate_rebalancing_actions(allocation)
            
            return {
                "portfolio_allocation": allocation,
                "actions": actions,
                "rationale": f"{investment_style} ì„±í–¥ ê¸°ë°˜ ìµœì í™”ëœ í¬íŠ¸í´ë¦¬ì˜¤ ì „ëµ",
                "expected_return": self._calculate_expected_return(allocation),
                "expected_volatility": self._calculate_expected_volatility(allocation),
                "risk_level": investment_style,
                "generated_at": datetime.now().isoformat(),
                "strategy_type": "enhanced_rule_based"
            }
            
        except Exception as e:
            logger.error(f"Error in enhanced rule-based strategy: {str(e)}")
            return self._generate_fallback_strategy(user_profile)

    def _calculate_market_adjustment(self, processed_data: pd.DataFrame) -> Dict[str, float]:
        """ì‹œì¥ ìƒí™© ë¶„ì„ ë° ì¡°ì • ê³„ìˆ˜ ê³„ì‚°"""
        try:
            # ì „ì²´ ì‹œì¥ íŠ¸ë Œë“œ ê³„ì‚°
            market_returns = processed_data.groupby('Symbol')['Daily_Return'].mean()
            market_volatility = processed_data.groupby('Symbol')['Volatility_30D'].mean()
            
            # RSI ê¸°ë°˜ ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ íŒë‹¨
            avg_rsi = processed_data.groupby('Symbol')['RSI'].last().mean()
            
            # ì¡°ì • ê³„ìˆ˜ ê³„ì‚°
            adjustment = {
                "growth_bias": 1.1 if market_returns.mean() > 0.005 else 0.9,  # ìƒìŠ¹ì¥ì´ë©´ ì„±ì¥ì£¼ ë¹„ì¤‘ ì¦ê°€
                "defensive_bias": 1.1 if avg_rsi > 70 else 0.9,  # ê³¼ë§¤ìˆ˜ì‹œ ë°©ì–´ì£¼ ë¹„ì¤‘ ì¦ê°€
                "volatility_factor": 0.9 if market_volatility.mean() > 0.03 else 1.0
            }
            
            return adjustment
            
        except Exception as e:
            logger.error(f"Error calculating market adjustment: {str(e)}")
            return {"growth_bias": 1.0, "defensive_bias": 1.0, "volatility_factor": 1.0}

    def _adjust_allocation_for_market(
        self, 
        base_allocation: Dict[str, float], 
        adjustment: Dict[str, float]
    ) -> Dict[str, float]:
        """ì‹œì¥ ìƒí™©ì„ ë°˜ì˜í•œ ë°°ë¶„ ì¡°ì •"""
        adjusted_allocation = base_allocation.copy()
        
        growth_stocks = ["NVIDIA", "Tesla", "Amazon", "ì„±ì¥ì£¼ETF"]
        defensive_stocks = ["Johnson & Johnson", "Coca-Cola", "ì±„ê¶ŒETF", "í˜„ê¸ˆ"]
        
        for stock in adjusted_allocation:
            if any(growth in stock for growth in growth_stocks):
                adjusted_allocation[stock] *= adjustment["growth_bias"]
            elif any(defensive in stock for defensive in defensive_stocks):
                adjusted_allocation[stock] *= adjustment["defensive_bias"]
            
            adjusted_allocation[stock] *= adjustment["volatility_factor"]
        
        # ì •ê·œí™”í•˜ì—¬ ì´í•©ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •
        total = sum(adjusted_allocation.values())
        if total > 0:
            adjusted_allocation = {k: v/total for k, v in adjusted_allocation.items()}
        
        return adjusted_allocation

    def _adjust_for_user_insights(
        self, 
        allocation: Dict[str, float], 
        user_insights: Dict[str, Any]
    ) -> Dict[str, float]:
        """ì‚¬ìš©ì ì¸ì‚¬ì´íŠ¸ë¥¼ ë°˜ì˜í•œ ë°°ë¶„ ì¡°ì •"""
        # ì‚¬ìš©ìì˜ ì„ í˜¸ë„ë‚˜ íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ ë°˜ì˜
        # ì´ ë¶€ë¶„ì€ ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ NLP ë¶„ì„ì„ í†µí•´ êµ¬í˜„ë  ê²ƒ
        
        key_learnings = user_insights.get("key_learnings", [])
        
        for learning in key_learnings:
            learning_lower = learning.lower()
            
            # ESG íˆ¬ì ì„ í˜¸
            if 'esg' in learning_lower or 'ì§€ì†ê°€ëŠ¥' in learning_lower:
                allocation["Apple"] = allocation.get("Apple", 0) * 1.2
                allocation["Microsoft"] = allocation.get("Microsoft", 0) * 1.2
            
            # ê¸°ìˆ ì£¼ ì„ í˜¸
            if 'ê¸°ìˆ ' in learning_lower or 'tech' in learning_lower:
                allocation["NVIDIA"] = allocation.get("NVIDIA", 0) * 1.3
                allocation["Apple"] = allocation.get("Apple", 0) * 1.2
            
            # ì•ˆì •ì„± ì¤‘ì‹œ
            if 'ì•ˆì •' in learning_lower or 'ë³´ìˆ˜' in learning_lower:
                allocation["ì±„ê¶ŒETF"] = allocation.get("ì±„ê¶ŒETF", 0) * 1.3
                allocation["í˜„ê¸ˆ"] = allocation.get("í˜„ê¸ˆ", 0) * 1.2
        
        # ì •ê·œí™”
        total = sum(allocation.values())
        if total > 0:
            allocation = {k: v/total for k, v in allocation.items()}
        
        return allocation

    def _generate_rebalancing_actions(self, target_allocation: Dict[str, float]) -> List[Dict[str, Any]]:
        """ë¦¬ë°¸ëŸ°ì‹± ì•¡ì…˜ ìƒì„±"""
        actions = []
        
        for stock, target_weight in target_allocation.items():
            if target_weight > 0.05:  # 5% ì´ìƒì¸ ì¢…ëª©ë§Œ
                actions.append({
                    "action": "ë§¤ìˆ˜" if target_weight > 0.1 else "ì†ŒëŸ‰ë§¤ìˆ˜",
                    "stock": stock,
                    "target_weight": target_weight,
                    "reason": f"ëª©í‘œ ë¹„ì¤‘ {target_weight:.1%} ë‹¬ì„±ì„ ìœ„í•œ ì¡°ì •",
                    "priority": "high" if target_weight > 0.15 else "medium"
                })
        
        return sorted(actions, key=lambda x: x["target_weight"], reverse=True)

    def _calculate_expected_return(self, allocation: Dict[str, float]) -> str:
        """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆìƒ ìˆ˜ìµë¥  ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì¶”ì • ëª¨ë¸ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ ì‚¬ìš©)
        expected_returns = {
            "NVIDIA": 0.25, "Tesla": 0.22, "Apple": 0.15, "Microsoft": 0.14,
            "Amazon": 0.18, "Google": 0.16, "ì‚¼ì„±ì „ì": 0.12, "SKí•˜ì´ë‹‰ìŠ¤": 0.15,
            "NAVER": 0.10, "TSMC": 0.18, "ì±„ê¶ŒETF": 0.04, "í˜„ê¸ˆ": 0.02
        }
        
        weighted_return = 0
        for stock, weight in allocation.items():
            stock_return = expected_returns.get(stock, 0.10)  # ê¸°ë³¸ê°’ 10%
            weighted_return += weight * stock_return
        
        lower_bound = max(0.05, weighted_return - 0.03)
        upper_bound = weighted_return + 0.03
        
        return f"{lower_bound:.1%}-{upper_bound:.1%}"

    def _calculate_expected_volatility(self, allocation: Dict[str, float]) -> str:
        """í¬íŠ¸í´ë¦¬ì˜¤ ì˜ˆìƒ ë³€ë™ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ ì¶”ì • ëª¨ë¸
        volatilities = {
            "NVIDIA": 0.35, "Tesla": 0.40, "Apple": 0.25, "Microsoft": 0.22,
            "Amazon": 0.28, "Google": 0.24, "ì‚¼ì„±ì „ì": 0.20, "SKí•˜ì´ë‹‰ìŠ¤": 0.25,
            "NAVER": 0.22, "TSMC": 0.30, "ì±„ê¶ŒETF": 0.05, "í˜„ê¸ˆ": 0.01
        }
        
        weighted_volatility = 0
        for stock, weight in allocation.items():
            stock_vol = volatilities.get(stock, 0.20)  # ê¸°ë³¸ê°’ 20%
            weighted_volatility += weight * (stock_vol ** 2)
        
        portfolio_volatility = np.sqrt(weighted_volatility)
        
        return f"{portfolio_volatility:.1%}"

    async def _validate_and_optimize_strategy(
        self, 
        strategy: Dict[str, Any], 
        processed_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """ì „ëµ ê²€ì¦ ë° ìµœì í™”"""
        try:
            # 1. ë°°ë¶„ í•©ê³„ ê²€ì¦
            allocation = strategy.get("portfolio_allocation", {})
            total_weight = sum(allocation.values())
            
            if abs(total_weight - 1.0) > 0.01:  # 1% ì˜¤ì°¨ í—ˆìš©
                # ì •ê·œí™”
                normalized_allocation = {k: v/total_weight for k, v in allocation.items()} if total_weight > 0 else allocation
                strategy["portfolio_allocation"] = normalized_allocation
                strategy["validation_notes"] = [f"í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ ì •ê·œí™” ì™„ë£Œ (ì›ë˜ í•©ê³„: {total_weight:.2%})"]
            
            # 2. ìµœì†Œ/ìµœëŒ€ ë¹„ì¤‘ ì œí•œ
            optimized_allocation = {}
            for stock, weight in strategy["portfolio_allocation"].items():
                # ìµœì†Œ 1%, ìµœëŒ€ 30% ì œí•œ
                optimized_weight = max(0.01, min(0.30, weight))
                optimized_allocation[stock] = optimized_weight
            
            # 3. ì¬ì •ê·œí™”
            total_optimized = sum(optimized_allocation.values())
            strategy["portfolio_allocation"] = {k: v/total_optimized for k, v in optimized_allocation.items()}
            
            # 4. ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
            diversification_score = len([w for w in strategy["portfolio_allocation"].values() if w > 0.05])
            strategy["diversification_score"] = diversification_score
            
            # 5. ìµœì í™” ë…¸íŠ¸ ì¶”ê°€
            optimization_notes = strategy.get("validation_notes", [])
            optimization_notes.append(f"ë‹¤ì–‘ì„± ì ìˆ˜: {diversification_score} (5% ì´ìƒ ì¢…ëª© ìˆ˜)")
            
            if diversification_score < 5:
                optimization_notes.append("ê¶Œì¥: ë” ë§ì€ ì¢…ëª©ìœ¼ë¡œ ë¶„ì‚° íˆ¬ì ê³ ë ¤")
            
            strategy["validation_notes"] = optimization_notes
            strategy["validated_at"] = datetime.now().isoformat()
            
            return strategy
            
        except Exception as e:
            logger.error(f"Error validating strategy: {str(e)}")
            return strategy

    async def _perform_risk_analysis(
        self, 
        strategy: Dict[str, Any], 
        processed_data: pd.DataFrame
    ) -> Dict[str, Any]:
        """ë¦¬ìŠ¤í¬ ë¶„ì„ ìˆ˜í–‰"""
        try:
            allocation = strategy.get("portfolio_allocation", {})
            
            # 1. ì§‘ì¤‘ë„ ë¦¬ìŠ¤í¬
            max_weight = max(allocation.values()) if allocation else 0
            concentration_risk = "ë†’ìŒ" if max_weight > 0.25 else "ë³´í†µ" if max_weight > 0.15 else "ë‚®ìŒ"
            
            # 2. ì„¹í„° ì§‘ì¤‘ë„
            tech_weight = sum(
                weight for stock, weight in allocation.items()
                if any(tech in stock for tech in ["Apple", "NVIDIA", "Microsoft", "Amazon", "Google", "NAVER", "ì‚¼ì„±ì „ì"])
            )
            sector_concentration = "ë†’ìŒ" if tech_weight > 0.6 else "ë³´í†µ" if tech_weight > 0.4 else "ë‚®ìŒ"
            
            # 3. ìœ ë™ì„± ë¦¬ìŠ¤í¬ (ê°„ë‹¨í•œ ì¶”ì •)
            liquid_weight = sum(
                weight for stock, weight in allocation.items()
                if any(liquid in stock for liquid in ["Apple", "Microsoft", "ì‚¼ì„±ì „ì", "NVIDIA"])
            )
            liquidity_risk = "ë‚®ìŒ" if liquid_weight > 0.5 else "ë³´í†µ"
            
            # 4. í†µí™” ë¦¬ìŠ¤í¬
            foreign_weight = sum(
                weight for stock, weight in allocation.items()
                if not any(korean in stock for korean in ["ì‚¼ì„±", "SK", "NAVER", "ì¹´ì¹´ì˜¤", "í˜„ëŒ€"])
            )
            currency_risk = "ë†’ìŒ" if foreign_weight > 0.7 else "ë³´í†µ" if foreign_weight > 0.4 else "ë‚®ìŒ"
            
            # 5. ì „ì²´ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´
            risk_scores = {
                "ë†’ìŒ": 3, "ë³´í†µ": 2, "ë‚®ìŒ": 1
            }
            
            total_risk_score = (
                risk_scores[concentration_risk] + 
                risk_scores[sector_concentration] + 
                risk_scores[liquidity_risk] + 
                risk_scores[currency_risk]
            ) / 4
            
            overall_risk = "ë†’ìŒ" if total_risk_score > 2.5 else "ë³´í†µ" if total_risk_score > 1.5 else "ë‚®ìŒ"
            
            return {
                "concentration_risk": concentration_risk,
                "max_position_weight": f"{max_weight:.1%}",
                "sector_concentration": sector_concentration,
                "tech_sector_weight": f"{tech_weight:.1%}",
                "liquidity_risk": liquidity_risk,
                "currency_risk": currency_risk,
                "foreign_exposure": f"{foreign_weight:.1%}",
                "overall_risk_level": overall_risk,
                "risk_score": round(total_risk_score, 2),
                "risk_mitigation_suggestions": [
                    "ì •ê¸°ì ì¸ ë¦¬ë°¸ëŸ°ì‹± (ë¶„ê¸°ë³„ ê¶Œì¥)" if concentration_risk == "ë†’ìŒ" else None,
                    "ì„¹í„° ë¶„ì‚° í™•ëŒ€ ê³ ë ¤" if sector_concentration == "ë†’ìŒ" else None,
                    "í™˜í—¤ì§€ ì „ëµ ê²€í† " if currency_risk == "ë†’ìŒ" else None
                ]
            }
            
        except Exception as e:
            logger.error(f"Error performing risk analysis: {str(e)}")
            return {
                "overall_risk_level": "ë³´í†µ",
                "error": "ë¦¬ìŠ¤í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            }

    # Helper methods for data processing
    async def _search_google_for_learning(self, keyword: str) -> List[Dict[str, Any]]:
        """Google Search API for learning purposes"""
        if not self.google_search_api_key:
            return []
        
        try:
            url = "https://www.googleapis.com/customsearch/v1"
            params = {
                'key': self.google_search_api_key,
                'cx': self.google_search_engine_id,
                'q': f"{keyword} íˆ¬ì ì „ëµ ë¶„ì„ 2024",
                'num': 5,
                'lr': 'lang_ko'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return [
                            {
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'snippet': item.get('snippet', '')
                            }
                            for item in data.get('items', [])
                        ]
            return []
            
        except Exception as e:
            logger.error(f"Error in Google search: {str(e)}")
            return []

    async def _extract_web_content(self, url: str) -> Optional[Dict[str, Any]]:
        """ì›¹ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # Remove script and style elements
                        for script in soup(["script", "style"]):
                            script.extract()
                        
                        text = soup.get_text()
                        lines = (line.strip() for line in text.splitlines())
                        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                        clean_text = ' '.join(chunk for chunk in chunks if chunk)
                        
                        return {
                            "url": url,
                            "content": clean_text[:8000],  # ì²˜ìŒ 8000ì
                            "title": soup.find('title').get_text() if soup.find('title') else ""
                        }
            
            return None
            
        except Exception as e:
            logger.error(f"Error extracting web content from {url}: {str(e)}")
            return None

    async def _download_and_extract_ebook(self, url: str) -> Optional[str]:
        """E-book ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.read()
                        
                        # PDFì¸ ê²½ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if url.lower().endswith('.pdf'):
                            return self._extract_text_from_pdf_bytes(content)
                        else:
                            # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                            return content.decode('utf-8', errors='ignore')
            
            return None
            
        except Exception as e:
            logger.error(f"Error downloading ebook from {url}: {str(e)}")
            return None

    def _extract_text_from_pdf(self, pdf_content: bytes) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = io.BytesIO(pdf_content)
            reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            for page in reader.pages[:10]:  # ì²˜ìŒ 10í˜ì´ì§€ë§Œ
                text += page.extract_text() + "\n"
            
            return text
            
        except Exception as e:
            logger.error(f"Error extracting PDF text: {str(e)}")
            return ""

    def _extract_text_from_pdf_bytes(self, pdf_bytes: bytes) -> str:
        """PDF ë°”ì´íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = io.BytesIO(pdf_bytes)
            reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            for page in reader.pages[:10]:  # ì²˜ìŒ 10í˜ì´ì§€ë§Œ
                text += page.extract_text() + "\n"
            
            return text
            
        except Exception as e:
            logger.error(f"Error extracting PDF bytes text: {str(e)}")
            return ""

    async def _search_arxiv_papers(self, keyword: str) -> List[Dict[str, Any]]:
        """arXiv ë…¼ë¬¸ ê²€ìƒ‰"""
        try:
            search = arxiv.Search(
                query=f"all:{keyword} AND cat:q-fin*",
                max_results=5,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in search.results():
                papers.append({
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "summary": result.summary,
                    "url": result.entry_id,
                    "published": result.published.isoformat()
                })
            
            return papers
            
        except Exception as e:
            logger.error(f"Error searching arXiv papers: {str(e)}")
            return []

    async def _analyze_content_with_claude(self, content: str, analysis_prompt: str) -> str:
        """Claudeë¥¼ ì‚¬ìš©í•œ ì½˜í…ì¸  ë¶„ì„"""
        if not self.client:
            return "AI ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            prompt = f"""
ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

{analysis_prompt}

ì½˜í…ì¸ :
{content[:6000]}

í•µì‹¬ íˆ¬ì ì¸ì‚¬ì´íŠ¸ì™€ ì‹¤ìš©ì ì¸ ì „ëµì„ 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
            
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Error analyzing content with Claude: {str(e)}")
            return f"ì½˜í…ì¸  ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def _split_content_into_chunks(self, content: str, max_chunk_size: int = 8000) -> List[str]:
        """ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        current_chunk = ""
        
        sentences = content.split('. ')
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < max_chunk_size:
                current_chunk += sentence + '. '
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence + '. '
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks

    def _extract_key_learnings(self, insights: List[Dict[str, Any]]) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ì—ì„œ í•µì‹¬ í•™ìŠµ ë‚´ìš© ì¶”ì¶œ"""
        key_learnings = []
        
        for insight in insights[-5:]:  # ìµœê·¼ 5ê°œ
            analysis = insight.get("analysis", "")
            if analysis and len(analysis) > 50:
                # ì²« ë²ˆì§¸ ë¬¸ì¥ ë˜ëŠ” í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ
                first_sentence = analysis.split('.')[0] + '.'
                if len(first_sentence) > 20:
                    key_learnings.append(first_sentence)
        
        return key_learnings

    def _get_relevant_expert_strategies(self, investment_style: str) -> List[Dict[str, Any]]:
        """ê´€ë ¨ ì „ë¬¸ê°€ ì „ëµ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.expert_db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT expert_name, strategy_name, rationale, allocation_json, performance_metrics
                FROM expert_strategies 
                WHERE investment_style = ? OR investment_style = 'moderate'
                ORDER BY created_at DESC
                LIMIT 5
            ''', (investment_style,))
            
            results = cursor.fetchall()
            conn.close()
            
            strategies = []
            for result in results:
                strategies.append({
                    "expert_name": result[0],
                    "strategy_name": result[1],
                    "rationale": result[2],
                    "allocation": json.loads(result[3]) if result[3] else {},
                    "performance_metrics": json.loads(result[4]) if result[4] else {}
                })
            
            return strategies
            
        except Exception as e:
            logger.error(f"Error getting expert strategies: {str(e)}")
            return []

    def _analyze_market_trend(self, processed_data: pd.DataFrame) -> str:
        """ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            if processed_data.empty:
                return "ì‹œì¥ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
            # ì „ì²´ ì‹œì¥ í‰ê·  ìˆ˜ìµë¥ 
            avg_return = processed_data['Daily_Return'].mean()
            avg_volatility = processed_data['Volatility_30D'].mean()
            
            # RSI í‰ê· 
            avg_rsi = processed_data['RSI'].mean()
            
            # íŠ¸ë Œë“œ íŒë‹¨
            if avg_return > 0.01:
                trend = "ê°•í•œ ìƒìŠ¹"
            elif avg_return > 0.005:
                trend = "ìƒìŠ¹"
            elif avg_return > -0.005:
                trend = "íš¡ë³´"
            else:
                trend = "í•˜ë½"
            
            volatility_level = "ë†’ìŒ" if avg_volatility > 0.03 else "ë³´í†µ" if avg_volatility > 0.02 else "ë‚®ìŒ"
            market_sentiment = "ê³¼ë§¤ìˆ˜" if avg_rsi > 70 else "ê³¼ë§¤ë„" if avg_rsi < 30 else "ì¤‘ë¦½"
            
            return f"ì‹œì¥ íŠ¸ë Œë“œ: {trend}, ë³€ë™ì„±: {volatility_level}, ì‹¬ë¦¬: {market_sentiment}"
            
        except Exception as e:
            logger.error(f"Error analyzing market trend: {str(e)}")
            return "ì‹œì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _integrate_learning_results(self, training_results: Dict[str, Any]) -> Dict[str, Any]:
        """í•™ìŠµ ê²°ê³¼ í†µí•©"""
        integrated = {
            "total_insights": 0,
            "learning_sources": [],
            "key_themes": [],
            "confidence_score": 0.0
        }
        
        for source, results in training_results.items():
            if isinstance(results, dict) and not results.get("error"):
                if "insights_count" in results:
                    integrated["total_insights"] += results["insights_count"]
                    integrated["learning_sources"].append(source)
                
                if "key_learnings" in results:
                    integrated["key_themes"].extend(results["key_learnings"][:2])
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        integrated["confidence_score"] = min(1.0, integrated["total_insights"] / 20)
        
        return integrated

    def _generate_fallback_strategy(self, user_profile: Dict[str, Any]) -> Dict[str, Any]:
        """í´ë°± ì „ëµ ìƒì„±"""
        investment_style = user_profile.get('investment_style', 'moderate')
        
        fallback_allocations = {
            'conservative': {
                "ì‚¼ì„±ì „ì": 0.25, "Apple": 0.20, "Microsoft": 0.15,
                "Johnson & Johnson": 0.10, "ì±„ê¶ŒETF": 0.20, "í˜„ê¸ˆ": 0.10
            },
            'moderate': {
                "ì‚¼ì„±ì „ì": 0.20, "Apple": 0.18, "Microsoft": 0.15,
                "NVIDIA": 0.12, "Amazon": 0.10, "Google": 0.10,
                "NAVER": 0.08, "ì±„ê¶ŒETF": 0.07
            },
            'aggressive': {
                "NVIDIA": 0.22, "Tesla": 0.18, "Apple": 0.15,
                "Amazon": 0.12, "ì‚¼ì„±ì „ì": 0.13, "TSMC": 0.10,
                "ì„±ì¥ì£¼ETF": 0.10
            }
        }
        
        allocation = fallback_allocations.get(investment_style, fallback_allocations['moderate'])
        
        return {
            "portfolio_allocation": allocation,
            "actions": [
                {
                    "action": "ê²€í† ",
                    "stock": "ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤",
                    "reason": "ê¸°ë³¸ ì „ëµ ì ìš©, ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥"
                }
            ],
            "rationale": f"{investment_style} ì„±í–¥ì— ë§ì¶˜ ê¸°ë³¸ í¬íŠ¸í´ë¦¬ì˜¤ ì „ëµì…ë‹ˆë‹¤. AI ë¶„ì„ì´ ì œí•œì ì´ë¯€ë¡œ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
            "expected_return": "8-12%" if investment_style == 'conservative' else "12-18%",
            "risk_level": investment_style,
            "generated_at": datetime.now().isoformat(),
            "strategy_type": "fallback",
            "warning": "ì œí•œëœ ë¶„ì„ìœ¼ë¡œ ìƒì„±ëœ ê¸°ë³¸ ì „ëµì…ë‹ˆë‹¤."
        }

    def _create_fallback_parsed_response(self, ai_response: str) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ ìƒì„±"""
        return {
            "portfolio_allocation": {
                "ì‚¼ì„±ì „ì": 0.20,
                "Apple": 0.15,
                "Microsoft": 0.12,
                "NVIDIA": 0.10,
                "Amazon": 0.10,
                "Google": 0.08,
                "NAVER": 0.08,
                "ì±„ê¶ŒETF": 0.10,
                "í˜„ê¸ˆ": 0.07
            },
            "actions": [
                {
                    "action": "ê²€í† ",
                    "stock": "AI ì‘ë‹µ íŒŒì‹±",
                    "reason": "AI ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
                }
            ],
            "rationale": f"AI ì‘ë‹µ: {ai_response[:300]}...",
            "expected_return": "10-15%",
            "expected_volatility": "15-20%",
            "risk_level": "ì¤‘ê°„",
            "generated_at": datetime.now().isoformat(),
            "strategy_type": "ai_parsed_fallback",
            "parsing_error": True
        }